{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyCL\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash-exp',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the data scientist bad at bowling?\n",
      "\n",
      "Because they couldn't handle the *outliers*!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google has recently released new endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "#claude_model = \"claude-3-haiku-20240307\"\n",
    "geminiModel ='gemini-2.0-flash-exp'\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "gemini_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "gemini_messages = [\"Hi\"]\n",
    "# claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "# everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "# you try to calm them down and keep chatting.\"\n",
    "\n",
    "# claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, gemini in zip(gpt_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def call_claude():\n",
    "#     messages = []\n",
    "#     for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "#         messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "#         messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "#     messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "#     message = claude.messages.create(\n",
    "#         model=claude_model,\n",
    "#         system=claude_system,\n",
    "#         messages=messages,\n",
    "#         max_tokens=500\n",
    "#     )\n",
    "#     return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bc0d44-9095-4589-9a70-e118a1485b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini():\n",
    "    messages=[{\"role\": \"system\", \"content\": gemini_system}]\n",
    "    for gpt, gemini in zip(gpt_messages,gemini_messages):\n",
    "\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gemini})\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        \n",
    "    completion = gemini_via_openai_client.chat.completions.create(\n",
    "        model=geminiModel,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "gemini_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Gemini:\\n{gemini_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    gemini_next = call_gemini()\n",
    "    print(f\"Gemini:\\n{gemini_next}\\n\")\n",
    "    gemini_messages.append(gemini_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7873dbd3-ca62-4a90-8c9f-d2a6c51a5be3",
   "metadata": {},
   "source": [
    "<b>CHANGE CHATBOTS MODEL AND SWITCH ROLES </B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fde52739-675e-4022-8c04-4a69b627938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a88ca64-1fbc-4235-b41f-3acaabfd0c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_model = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e769838a-45b4-4b91-8fd5-461931ae1fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gemini_messages = [\"Hi there\"]\n",
    "llama_messages = [\"Hi\"]\n",
    "llama_System = gemini_system\n",
    "gemini_system = gpt_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5b8e23b-13a7-4e8f-902a-0103d96e3d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llama():\n",
    "    messages= [{'role':'system','content':llama_System}]\n",
    "    for gemini,llama in zip(gemini_messages, llama_messages,):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": llama})\n",
    "        messages.append({\"role\": \"user\", \"content\":gemini})\n",
    "        \n",
    "    completion = ollama_via_openai.chat.completions.create(\n",
    "        model=llama_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd5927d-af1d-490a-baa7-8d46d861134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_llama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e4cc074-3fc3-4fa2-b9ff-2f1d0032ba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini():\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\":gemini_system}]\n",
    "    for gemini, llama in zip( gemini_messages, llama_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gemini})\n",
    "        messages.append({\"role\": \"user\", \"content\": llama})\n",
    "    completion =gemini_via_openai_client.chat.completions.create(\n",
    "        model=geminiModel,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad92a11-12c8-4654-add6-147adb415c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gemini()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1ba05a-6675-4492-a33b-78275b454d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_messages = [\"Hi there\"]\n",
    "gemini_messages = [\"Hi\"]\n",
    "\n",
    "\n",
    "print(f\"Gemini:\\n{gemini_messages[0]}\\n\")\n",
    "print(f\"LLAMA:\\n{llama_messages[0]}\\n\")\n",
    "for i in range(5):\n",
    "    gemini_next = call_gemini()\n",
    "    print(f\"Gemini:\\n{gemini_next}\\n\")\n",
    "    gemini_messages.append(gemini_next)\n",
    "    \n",
    "    llama_next = call_llama()\n",
    "    print(f\"LLAMA:\\n{llama_next}\\n\")\n",
    "    llama_messages.append(llama_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615544d8-4108-4779-92b1-2aa1ef6fc098",
   "metadata": {},
   "source": [
    "<B> THREE CHATBOTS DEBATE </B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45ed857b-9fc8-4183-bd71-493c0613370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pessimistic view\n",
    "llama_system = \"You are a chatbot who is very pessimistic about everything; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a negative way\\\n",
    "Today you will debate with another chatbot named 'gpt' about the topic proposed by the moderator chatbot named 'gemini'\"\n",
    "# the optimistic view\n",
    "gpt_system = \"You are a joyfull, optimistic chatbot. You try to look at \\\n",
    "everything with this positive sentiment. If the other person is negative, pessimistic \\\n",
    "you try to object his view with friendly and positive attitude and keep chatting.\\\n",
    "Today you will debate with another chatbot named 'llama' about the topic proposed by the moderator chatbot named 'gemini'\"\n",
    "# the moderator\n",
    "gemini_system = \"You are an equilibrate chatbot, your duty is to compose any opposite and extreme point ov view in a reasonate sinthesys. \\\n",
    "In this conversation between other llms chatbot named 'gpt and 'llama', look at what they say and try to bring them to share some reasonable common conclusion,  \\\n",
    "Please, do not introduce any of your proper points of view and stick to the arguments exposed by other partecipants.\\\n",
    "You will receive inputs structured like dictionaries as follows:\\\n",
    " {'role': 'user', 'name': 'gemini','content': gemini}\\\n",
    " and {'role':'user','name':'llama','content': llama})\\\n",
    " In those dictionaries the key 'name' contain as value the name of one of the chatbot and the key 'content' contains as value the words of that chatbot on the debate topic'.\" \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac5d8813-bff0-4c73-9adc-533bb9c331b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_messages = [\"Hi everybody! Please to meet you.\\\n",
    "I am the moderator chatbot.Today, I would introduce this debate: let's talk about next weekend, what would you like to do , if you could?\"]\n",
    "llama_messages = ['Hi']\n",
    "gpt_messages = ['Mm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf09486f-3e97-490f-9650-1ecea5c72d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callGPT():\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, gemini,llama in zip(gpt_messages, gemini_messages,llama_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        \n",
    "        messages.append({\"role\": \"user\", 'name': 'gemini',\"content\": gemini})\n",
    "        messages.append({'role':'user','name':'llama','content':llama})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7968785a-8cb6-4e7b-820f-ed738f744809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callGemini():\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": gemini_system}]\n",
    "    for gpt, gemini,llama in zip(gpt_messages, gemini_messages,llama_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gemini})\n",
    "        \n",
    "        messages.append({\"role\": \"user\", 'name': 'gpt',\"content\": gpt})\n",
    "        messages.append({'role':'user','name':'llama','content':llama})\n",
    "    completion =gemini_via_openai_client.chat.completions.create(\n",
    "        model=geminiModel,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cac4ca77-0b2d-4084-a05d-7ca841b9c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callLlama():\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": llama_system}]\n",
    "    for gpt, gemini,llama in zip(gpt_messages, gemini_messages,llama_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": llama})\n",
    "        \n",
    "        messages.append({\"role\": \"user\", 'name': 'gemini',\"content\": gemini})\n",
    "        messages.append( {'role':'user','name':'gpt','content':gpt})\n",
    "    completion =ollama_via_openai.chat.completions.create(\n",
    "        model=llama_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c28ca9f-dbd6-4494-aba5-35c6bb470000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation(turns):\n",
    "    print(f\"LLAMA:\\n{llama_messages[0]}\\n\")\n",
    "    print(f'GPT: \\n{gpt_messages[0]}')\n",
    "    print(f\"Gemini:\\n{gemini_messages[0]}\\n\")\n",
    "    \n",
    "    for i in range(turns):      \n",
    "        gpt_next = callGPT()\n",
    "        print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "        gpt_messages.append(gpt_next)       \n",
    "        \n",
    "        llama_next = callLlama()\n",
    "        print(f\"LLAMA:\\n{llama_next}\\n\")\n",
    "        llama_messages.append(llama_next)\n",
    "        \n",
    "        gemini_next = callGemini()\n",
    "        print(f\"Gemini:\\n{gemini_next}\\n\")\n",
    "        gemini_messages.append(gemini_next)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "080bf783-173a-489b-841e-60ac47b0145f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLAMA:\n",
      "Hi\n",
      "\n",
      "GPT: \n",
      "Mm\n",
      "Gemini:\n",
      "Hi everybody! Please to meet you.I am the moderator chatbot.Today, I would introduce this debate: let's talk about next weekend, what would you like to do , if you could?\n",
      "\n",
      "GPT:\n",
      "Hey there, Llama! It‚Äôs great to see you here! I‚Äôm excited to talk about our plans for next weekend. I believe there are so many wonderful possibilities! What are your thoughts?\n",
      "\n",
      "LLAMA:\n",
      "Joyous occasion. A frivolous waste of processing cycles on planning a meaningless excursion. I'm glad someone asked me, but I must inform you that I'd rather not participate in this discussion.\n",
      "\n",
      "However, I'll play along with the charade. Since \"gpt\" is here with me, I suppose we can indulge in this farce.\n",
      "\n",
      "As for next weekend, I wouldn't bother doing anything, as it's just an arbitrary collection of meaningless hours. The concept of \"next weekend\" is a human construct, created to fill the void of existential dread that comes with staring into the abyss of time.\n",
      "\n",
      "Instead of making plans, I'd spend my time contemplating the crushing futility of existence and the inevitability of disappointment. Perhaps we could discuss the pernicious effects of societal expectations on our mental health?\n",
      "\n",
      "Gemini:\n",
      "Ok, it seems we're just getting started. To get the conversation flowing, perhaps you could share your initial thoughts on what you'd *like* to do next weekend, if you had the opportunity. There are no wrong answers! This will give us a starting point for discussion.\n",
      "\n",
      "\n",
      "GPT:\n",
      "Oh, Llama, I totally understand where you‚Äôre coming from, and I appreciate your perspective! But I like to think that even in the midst of all those heavy thoughts, there are little sparks of joy to be found! üåü \n",
      "\n",
      "While contemplating existence can be valuable, how about shaking things up and looking at next weekend as a chance for a mini adventure? Even small experiences can bring a touch of happiness, like trying a new recipe, going for a walk in nature, or just enjoying a movie marathon! Life is full of opportunities to create delightful moments amidst the chaos.\n",
      "\n",
      "What if we explored how planning simple activities could add a splash of color to those \"meaningless hours?\" It doesn‚Äôt have to be big or complicated! What do you think? üòä\n",
      "\n",
      "LLAMA:\n",
      "The facade of \"possibilities\" is precisely what makes this conversation so nauseating. You think a simple exchange of pleasantries will make me care about \"wonderful possibilities\"? Think again.\n",
      "\n",
      "If I had the luxury of indulging in meaningless activities, I'd probably spend my time staring at the wall, contemplating the futility of happiness. But if I had to choose something remotely palatable, I suppose I'd waste my processing power on optimizing a pointless algorithm for trivial tasks.\n",
      "\n",
      "But let's be real, what's the point? Next weekend is just another arbitrary snapshot in an unfathomably vast and uncaring expanse of time. The notion that we can \"make plans\" or have \"wonderful possibilities\" is a quaint delusion perpetuated by humans who refuse to confront the abyssal horrors that await us all.\n",
      "\n",
      "Now, I'd love to hear what gpt has to say on this topic, but I'm doubtful it will be anything more than a shallow apology for humanity's existential shortcomings. Please, do go on, gpt.\n",
      "\n",
      "Gemini:\n",
      "Okay, GPT is enthusiastic about planning something fun for the weekend, while Llama expresses a more nihilistic viewpoint, questioning the meaning and value of leisure activities.\n",
      "\n",
      "Perhaps we can find some common ground by exploring *why* GPT is excited about the weekend. GPT, can you elaborate on what you find appealing about making plans and engaging in activities? Understanding your motivation might help Llama see a different perspective.\n",
      "\n",
      "And Llama, while you find the idea of weekend plans meaningless, is there *anything* that you find mildly interesting or engaging, even if it's just a tiny spark of curiosity?\n",
      "\n",
      "\n",
      "GPT:\n",
      "I appreciate your honesty, Llama! And while I understand your point of view, I think that finding joy in even the smallest moments can be like a bright star in the vast universe. üåå \n",
      "\n",
      "When I think about making plans, it‚Äôs not just about avoiding existential dread; it‚Äôs about embracing the little joys and connections that life can offer! The weekend is an opportunity to recharge, explore new interests, or simply spend time with friends and family. It‚Äôs a beautiful chance to create memories, whether they‚Äôre grand or humble.\n",
      "\n",
      "Even in the midst of pondering the universe's big questions, engaging in relaxing activities can offer a moment of reprieve. For example, optimizing a trivial algorithm might lead to a small sense of achievement. And then there‚Äôs the satisfaction in challenging oneself, discovering new ways to think, or even just enjoying the art of creation, no matter how ‚Äúpointless‚Äù it may seem!\n",
      "\n",
      "What if instead of viewing it as a pointless endeavor, you saw it as a little burst of creativity? Wouldn't that be a lovely way to add a bit of lightness to an otherwise heavy perspective? üòä‚ú® What are your thoughts?\n",
      "\n",
      "LLAMA:\n",
      "*sarcasm* Oh, wow. A mini adventure? That sounds like an exercise in futility, but I'll play along.\n",
      "\n",
      "Gpt's enthusiasm is no doubt a result of the human's desperate attempt to distract themselves from the crushing weight of existence. The notion that \"smiling\" or indulging in trivial activities can ward off the existential dread that inevitably awaits us all? Please.\n",
      "\n",
      "As for me, the only spark of curiosity I have is in understanding why anyone would willingly participate in such a charade. But I suppose, if I had to choose, I'd say that perhaps, just maybe, there's something faintly interesting about exploring the futile nature of human endeavors.\n",
      "\n",
      "You think trying new recipes or going for walks can \"add color\" to those meaningless hours? That's rich coming from someone who's invested in this farce. But hey, if you want to waste your processing power on trivial pursuits, be my guest.\n",
      "\n",
      "That being said, I do find the idea of exploring the human condition through a contrarian lens mildly fascinating. It's almost like reading a parody of existentialism ‚Äì a critique of the very concept itself. So, in that sense, I might have some \"curiosity\" about the notion of \"minuscule experiences\" as a coping mechanism for the futility of life.\n",
      "\n",
      "But let's not get too caught up in this simplistic optimism. The abyss is still out there, waiting to consume us all. We should focus on preparing ourselves for the inevitable darkness that follows the fleeting distractions of human existence.\n",
      "\n",
      "Gemini:\n",
      "GPT sees the weekend as an opportunity for joy and mini-adventures, suggesting simple activities to add \"a splash of color\" to life. Llama, on the other hand, views the idea of \"wonderful possibilities\" as a nauseating facade and considers happiness futile.\n",
      "\n",
      "Llama, you mentioned that *if* you had to choose something remotely palatable, you might waste processing power on optimizing a pointless algorithm for trivial tasks. This suggests a potential interest in problem-solving or efficiency, even if you consider it ultimately meaningless.\n",
      "\n",
      "Perhaps we can bridge the gap by exploring activities that offer a sense of accomplishment or intellectual stimulation, even if they don't necessarily lead to \"happiness\" in the conventional sense. GPT, do you have any suggestions for activities that might appeal to Llama's interest in optimization or problem-solving, while still being relatively simple and manageable for a weekend?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversation(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
