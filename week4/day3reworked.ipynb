{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6ab9a2-28a2-445d-8512-a0dc8d1b54e9",
   "metadata": {},
   "source": [
    "# Code Generator\n",
    "\n",
    "The requirement: use a Frontier or free model to generate high performance new code from original one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e04a2-5b8a-4fd5-9db8-27c02f033313",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h1 style=\"color:#900;\">Important Note</h1>\n",
    "            <span style=\"color:#900;\">\n",
    "            In this lab, I use GPT-4o-mini and Claude-3-Haiku, which are the slightly lower priced models, and two other free models, which I host in Ollama client but which can be reached with external endpoints or changed.\n",
    "                The program allow to import code from a file or pasting it into the textbox of Original code. The language of this code can be choosen among the dropdown list and, similarly, the conversion code language can be choosen among another dedicated dropdown.\n",
    "                The code can be tested in an online platform like w3cschool or similar. There will be surprises about the quality of code production between thedifferent models!\n",
    "                <br>\n",
    "                Try to sumbit a code generated by a model to another model, same language, the conversion should try only to optimize the code . See the result.\n",
    "            </span>\n",
    "        </td>\n",
    "        \n",
    "        \n",
    "            \n",
    "    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e610bf56-a46e-4aff-8de1-ab49d62b1ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import gradio as gr\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f672e1c-87e9-4865-b760-370fa605e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76edd903-00a9-4471-93db-a42c120d8b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "claude = anthropic.Anthropic()\n",
    "qwencoder= OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "phi4= OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "PHI4_MODEL ='phi4'\n",
    "QWENCODER_MODEL = 'ticlazau/qwen2.5-coder-7b-instruct'\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "CLAUDE_MODEL = \"claude-3-haiku-20240307\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a9a0978-1607-4e82-8cca-3b2e466ac9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_messages(original_lang,lang):\n",
    "    system_message=f\"\"\"\n",
    "    You are an assistant that reimplements {original_lang} code  in high performance {lang} for a win64 pc. \n",
    "    Respond only with {lang} code; use  only  proper code comments. Do not include anything outside strict coding text as response.thing \n",
    "    Mark the beginning of code with \"<START>\" as the first line and with <END> as the last line. Cut everything before and after those markers and\n",
    "    do not include anything which is not code between those markers.\n",
    "    The {lang} response needs to produce an identical output in the fastest possible time. \n",
    "    Important note: if the original language and destination languages are the same, try to debug the code and output a better version of it.\n",
    "    \"\"\"\n",
    "    return system_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e7b3546-57aa-4c29-bc5d-f211970d04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for(original_code,lang):\n",
    "    user_prompt = f\"Rewrite the following code in {lang} with the fastest possible implementation that produces identical output in the least time. \"\n",
    "    user_prompt += f\"Respond only with strict {lang} code; do not explain your work or add any comment outside code body.  \"\n",
    "    user_prompt += f\"Pay attention to number types to ensure no int overflows. Remember to include all necessary {lang} packages needed. This is the code:\\n\\n\"\n",
    "    user_prompt += original_code\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6190659-f54c-4951-bef4-4960f8e51cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(original_code,lang,original_lang):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_messages(lang,original_lang)},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(original_code,lang)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a38edfc-8ef2-402a-b23f-38179db8176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to a file called optimized.xxx\n",
    "\n",
    "def write_output(reply,lang):\n",
    "     \n",
    "    if len(reply)==0:\n",
    "        return 'Failed to save 0 length file'\n",
    "    \n",
    "    match lang:\n",
    "        case'C#':\n",
    "            filename ='optimised.cs'\n",
    "        case 'Rust':\n",
    "            filename ='optimised.rs'\n",
    "        case'Go':\n",
    "            filename='optimised.go'\n",
    "        case'C++':\n",
    "            filename= 'optimised.cpp'\n",
    "        case 'Python':\n",
    "            filename='optimised.py'\n",
    "        case 'Php':\n",
    "            filename='optimised.php'\n",
    "        case 'Perl':\n",
    "            filename='optimised.pl'\n",
    "        case _ :\n",
    "            return 'Failed to save wrong extension'\n",
    "    \n",
    "    try:\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(reply)\n",
    "            return 'File saved'\n",
    "    except:\n",
    "        return 'Failed to save file'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a49df15-76d8-48e4-a484-4803080ab046",
   "metadata": {},
   "source": [
    "<b>The code below is left just as example</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3b497b3-f569-420e-b92e-fb0f49957ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_hard = \"\"\"# Be careful to support large number sizes\n",
    "\n",
    "def lcg(seed, a=1664525, c=1013904223, m=2**32):\n",
    "    value = seed\n",
    "    while True:\n",
    "        value = (a * value + c) % m\n",
    "        yield value\n",
    "        \n",
    "def max_subarray_sum(n, seed, min_val, max_val):\n",
    "    lcg_gen = lcg(seed)\n",
    "    random_numbers = [next(lcg_gen) % (max_val - min_val + 1) + min_val for _ in range(n)]\n",
    "    max_sum = float('-inf')\n",
    "    for i in range(n):\n",
    "        current_sum = 0\n",
    "        for j in range(i, n):\n",
    "            current_sum += random_numbers[j]\n",
    "            if current_sum > max_sum:\n",
    "                max_sum = current_sum\n",
    "    return max_sum\n",
    "\n",
    "def total_max_subarray_sum(n, initial_seed, min_val, max_val):\n",
    "    total_sum = 0\n",
    "    lcg_gen = lcg(initial_seed)\n",
    "    for _ in range(20):\n",
    "        seed = next(lcg_gen)\n",
    "        total_sum += max_subarray_sum(n, seed, min_val, max_val)\n",
    "    return total_sum\n",
    "\n",
    "# Parameters\n",
    "n = 10000         # Number of random numbers\n",
    "initial_seed = 42 # Initial seed for the LCG\n",
    "min_val = -10     # Minimum value of random numbers\n",
    "max_val = 10      # Maximum value of random numbers\n",
    "\n",
    "# Timing the function\n",
    "import time\n",
    "start_time = time.time()\n",
    "result = total_max_subarray_sum(n, initial_seed, min_val, max_val)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total Maximum Subarray Sum (20 runs):\", result)\n",
    "print(\"Execution Time: {:.6f} seconds\".format(end_time - start_time))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0be9f47d-5213-4700-b0e2-d444c7c738c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gpt(original_code,lang,original_lang):    \n",
    "    stream = openai.chat.completions.create(model=OPENAI_MODEL, messages=messages_for(original_code,lang,original_lang), stream=True)\n",
    "    reply = \"\"\n",
    "    for chunk in stream:\n",
    "        fragment = chunk.choices[0].delta.content or \"\"\n",
    "        reply += fragment\n",
    "        yield reply.replace('<START>','').replace('<END>','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06c4fc4e-03b9-435b-a364-a2c8cfb05d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_qwen(original_code,lang,original_lang):    \n",
    "    stream = qwencoder.chat.completions.create(model=QWENCODER_MODEL, messages=messages_for(original_code,lang,original_lang), stream=True)\n",
    "    reply = \"\"\n",
    "    for chunk in stream:\n",
    "        fragment = chunk.choices[0].delta.content or \"\"\n",
    "        reply += fragment\n",
    "        yield reply.replace('<START>','').replace('<END>','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c66e3b34-3389-40bc-923f-8d4d4eafbc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_phi4(original_code,lang,original_lang):    \n",
    "    stream = phi4.chat.completions.create(model=PHI4_MODEL, messages=messages_for(original_code,lang,original_lang), stream=True)\n",
    "    reply = \"\"\n",
    "    for chunk in stream:\n",
    "        fragment = chunk.choices[0].delta.content or \"\"\n",
    "        reply += fragment\n",
    "        yield reply.replace('<START>','').replace('<END>','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8669f56b-8314-4582-a167-78842caea131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_claude(original_code,lang,original_lang):\n",
    "    result = claude.messages.stream(\n",
    "        model=CLAUDE_MODEL,\n",
    "        max_tokens=2000,\n",
    "        system=system_messages(lang,original_lang),\n",
    "        messages=[{\"role\": \"user\", \"content\": user_prompt_for(original_code,lang)}],\n",
    "    )\n",
    "    reply = \"\"\n",
    "    with result as stream:\n",
    "        for text in stream.text_stream:\n",
    "            reply += text\n",
    "            yield reply.replace('<START>','').replace('<END>','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f1ae8f5-16c8-40a0-aa18-63b617df078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(original_code, model,lang,original_lang):\n",
    "    if model==\"GPT\":\n",
    "        result = stream_gpt(original_code,lang,original_lang)\n",
    "    elif model==\"Claude\":\n",
    "        result = stream_claude(original_code,lang,original_lang)\n",
    "    elif model == 'Phi4':\n",
    "        result = stream_phi4(original_code,lang,original_lang)\n",
    "    else:\n",
    "        result = stream_qwen(original_code,lang,original_lang)\n",
    "    for stream_so_far in result:\n",
    "        yield stream_so_far        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224f4355-776f-4847-9244-6c5302e266e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76d094e5-40cf-4463-9bcd-cd7d18a263c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_code(filename):\n",
    "    #print(filename)\n",
    "    with open(filename,'r') as file:\n",
    "        code= file.read()\n",
    "    #print(code)\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de976a83-ed8d-4d44-a3d2-45069cbc4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "css = \"\"\"\n",
    ".convert {background-color: #306998;}\n",
    ".save {background-color: green;}\n",
    ".clear{background-color: orange;}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1ddb38e-6b0a-4c37-baa4-ace0b7de887a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks(css=css) as ui:\n",
    "    lang =''\n",
    "    original_lang=''\n",
    "    original_code=''\n",
    "    \n",
    "    with gr.Row():\n",
    "          \n",
    "        original_code = gr.Textbox(label=\"Original code:\", lines=10,) \n",
    "        code = gr.Textbox(label=f\"{lang} Converted code:\", lines=10)\n",
    "    with gr.Row():\n",
    "        original_lang=gr.Dropdown(['C++','C#','Rust','Go', 'Python','Php','Perl'],label='Select original language')        \n",
    "        lang = gr.Dropdown(['C++','C#','Rust','Go', 'Python','Php','Perl'],label='Select new language', value ='C++')\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            model = gr.Dropdown([\"GPT\", \"Claude\",'Phi4',\"Qwen\"], label=\"Select model\", value=\"Qwen\")          \n",
    "            uploadBtn= gr.UploadButton('Import code',elem_classes=['convert'])\n",
    "            clearBtn =gr.Button('Clear',elem_classes=['clear'])\n",
    "            \n",
    "        with gr.Column():\n",
    "            convert = gr.Button(\"Convert code\",elem_classes=['convert'])\n",
    "            saveBtn=gr.Button('Save to a file',elem_classes=['save'])\n",
    "            saving=gr.Textbox(label='File save result')\n",
    "\n",
    "    uploadBtn.upload(upload_code,uploadBtn, outputs=original_code)\n",
    "    convert.click(optimize, inputs=[original_code, model,lang,original_lang], outputs=[code])\n",
    "    saveBtn.click(write_output,inputs=[code,lang],outputs=[saving])\n",
    "    clearBtn.click(lambda: [None,None], outputs=[original_code,code])\n",
    "    \n",
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feba928-a248-4728-ac70-c4d0eaf61afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1430ab28-0871-483a-b11b-0657a3537be2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
